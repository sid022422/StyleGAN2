{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAHfPTJcqtl3",
        "outputId": "0fd6accd-51ca-49f8-b4d3-0a4981efd864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: labml in /usr/local/lib/python3.8/dist-packages (0.4.161)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.8/dist-packages (from labml) (3.1.29)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from labml) (1.23.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from labml) (6.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython->labml) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install labml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58MabOEhqz1T",
        "outputId": "f2109341-f0ed-4e62-e3a1-38a08dd3a809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: labml_helpers in /usr/local/lib/python3.8/dist-packages (0.4.89)\n",
            "Requirement already satisfied: labml>=0.4.158 in /usr/local/lib/python3.8/dist-packages (from labml_helpers) (0.4.161)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from labml_helpers) (1.13.0+cu116)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.8/dist-packages (from labml>=0.4.158->labml_helpers) (3.1.29)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from labml>=0.4.158->labml_helpers) (6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from labml>=0.4.158->labml_helpers) (1.23.5)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython->labml>=0.4.158->labml_helpers) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.158->labml_helpers) (5.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->labml_helpers) (4.4.0)\n"
          ]
        }
      ],
      "source": [
        "pip install labml_helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eavXpTejq1ew",
        "outputId": "8e9b4d20-2858-483b-bd40-33b60b983a1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: labml_nn in /usr/local/lib/python3.8/dist-packages (0.4.133)\n",
            "Requirement already satisfied: labml>=0.4.158 in /usr/local/lib/python3.8/dist-packages (from labml_nn) (0.4.161)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from labml_nn) (1.13.0+cu116)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.8/dist-packages (from labml_nn) (0.6.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.8/dist-packages (from labml_nn) (0.14.0)\n",
            "Requirement already satisfied: fairscale in /usr/local/lib/python3.8/dist-packages (from labml_nn) (0.4.12)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from labml_nn) (0.14.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from labml_nn) (1.23.5)\n",
            "Requirement already satisfied: labml-helpers>=0.4.89 in /usr/local/lib/python3.8/dist-packages (from labml_nn) (0.4.89)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.8/dist-packages (from labml>=0.4.158->labml_nn) (3.1.29)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from labml>=0.4.158->labml_nn) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->labml_nn) (4.4.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython->labml>=0.4.158->labml_nn) (4.0.10)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.158->labml_nn) (5.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext->labml_nn) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext->labml_nn) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext->labml_nn) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext->labml_nn) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext->labml_nn) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext->labml_nn) (2.10)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->labml_nn) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install labml_nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhK59E_0rJmW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "---\n",
        "title: StyleGAN 2\n",
        "summary: >\n",
        " An annotated PyTorch implementation of StyleGAN2.\n",
        "---\n",
        "# StyleGAN 2\n",
        "This is a [PyTorch](https://pytorch.org) implementation of the paper\n",
        " [Analyzing and Improving the Image Quality of StyleGAN](https://papers.labml.ai/paper/1912.04958)\n",
        " which introduces **StyleGAN 2**.\n",
        "StyleGAN 2 is an improvement over **StyleGAN** from the paper\n",
        " [A Style-Based Generator Architecture for Generative Adversarial Networks](https://papers.labml.ai/paper/1812.04948).\n",
        "And StyleGAN is based on **Progressive GAN** from the paper\n",
        " [Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://papers.labml.ai/paper/1710.10196).\n",
        "All three papers are from the same authors from [NVIDIA AI](https://twitter.com/NVIDIAAI).\n",
        "*Our implementation is a minimalistic StyleGAN 2 model training code.\n",
        "Only single GPU training is supported to keep the implementation simple.\n",
        "We managed to shrink it to keep it at less than 500 lines of code, including the training loop.*\n",
        "**üèÉ Here's the training code: [`experiment.py`](experiment.html).**\n",
        "![Generated Images](generated_64.png)\n",
        "---*These are $64 \\times 64$ images generated after training for about 80K steps.*---\n",
        "We'll first introduce the three papers at a high level.\n",
        "## Generative Adversarial Networks\n",
        "Generative adversarial networks have two components; the generator and the discriminator.\n",
        "The generator network takes a random latent vector ($z \\in \\mathcal{Z}$)\n",
        " and tries to generate a realistic image.\n",
        "The discriminator network tries to differentiate the real images from generated images.\n",
        "When we train the two networks together the generator starts generating images indistinguishable from real images.\n",
        "## Progressive GAN\n",
        "Progressive GAN generates high-resolution images ($1080 \\times 1080$) of size.\n",
        "It does so by *progressively* increasing the image size.\n",
        "First, it trains a network that produces a $4 \\times 4$ image, then $8 \\times 8$ ,\n",
        " then an $16 \\times 16$  image, and so on up to the desired image resolution.\n",
        "At each resolution, the generator network produces an image in latent space which is converted into RGB,\n",
        "with a $1 \\times 1$  convolution.\n",
        "When we progress from a lower resolution to a higher resolution\n",
        " (say from $4 \\times 4$  to $8 \\times 8$ ) we scale the latent image by $2\\times$\n",
        " and add a new block (two $3 \\times 3$  convolution layers)\n",
        " and a new $1 \\times 1$  layer to get RGB.\n",
        "The transition is done smoothly by adding a residual connection to\n",
        " the $2\\times$ scaled $4 \\times 4$  RGB image.\n",
        "The weight of this residual connection is slowly reduced, to let the new block take over.\n",
        "The discriminator is a mirror image of the generator network.\n",
        "The progressive growth of the discriminator is done similarly.\n",
        "![progressive_gan.svg](progressive_gan.svg)\n",
        "---*$2\\times$ and $0.5\\times$ denote feature map resolution scaling and scaling.\n",
        "$4\\times4$, $8\\times4$, ... denote feature map resolution at the generator or discriminator block.\n",
        "Each discriminator and generator block consists of 2 convolution layers with leaky ReLU activations.*---\n",
        "They use **minibatch standard deviation** to increase variation and\n",
        " **equalized learning rate** which we discussed below in the implementation.\n",
        "They also use **pixel-wise normalization** where at each pixel the feature vector is normalized.\n",
        "They apply this to all the convolution layer outputs (except RGB).\n",
        "## StyleGAN\n",
        "StyleGAN improves the generator of Progressive GAN keeping the discriminator architecture the same.\n",
        "#### Mapping Network\n",
        "It maps the random latent vector ($z \\in \\mathcal{Z}$)\n",
        " into a different latent space ($w \\in \\mathcal{W}$),\n",
        " with an 8-layer neural network.\n",
        "This gives an intermediate latent space $\\mathcal{W}$\n",
        "where the factors of variations are more linear (disentangled).\n",
        "#### AdaIN\n",
        "Then $w$ is transformed into two vectors (**styles**) per layer,\n",
        " $i$, $y_i = (y_{s,i}, y_{b,i}) = f_{A_i}(w)$ and used for scaling and shifting (biasing)\n",
        " in each layer with $\\text{AdaIN}$ operator (normalize and scale):\n",
        "$$\\text{AdaIN}(x_i, y_i) = y_{s, i} \\frac{x_i - \\mu(x_i)}{\\sigma(x_i)} + y_{b,i}$$\n",
        "#### Style Mixing\n",
        "To prevent the generator from assuming adjacent styles are correlated,\n",
        " they randomly use different styles for different blocks.\n",
        "That is, they sample two latent vectors $(z_1, z_2)$ and corresponding $(w_1, w_2)$ and\n",
        " use $w_1$ based styles for some blocks and $w_2$ based styles for some blacks randomly.\n",
        "#### Stochastic Variation\n",
        "Noise is made available to each block which helps the generator create more realistic images.\n",
        "Noise is scaled per channel by a learned weight.\n",
        "#### Bilinear Up and Down Sampling\n",
        "All the up and down-sampling operations are accompanied by bilinear smoothing.\n",
        "![style_gan.svg](style_gan.svg)\n",
        "---*$A$ denotes a linear layer.\n",
        "$B$ denotes a broadcast and scaling operation (noise is a single channel).\n",
        "StyleGAN also uses progressive growing like Progressive GAN.*---\n",
        "## StyleGAN 2\n",
        "StyleGAN 2 changes both the generator and the discriminator of StyleGAN.\n",
        "#### Weight Modulation and Demodulation\n",
        "They remove the $\\text{AdaIN}$ operator and replace it with\n",
        " the weight modulation and demodulation step.\n",
        "This is supposed to improve what they call droplet artifacts that are present in generated images,\n",
        " which are caused by the normalization in $\\text{AdaIN}$ operator.\n",
        "Style vector per layer is calculated from $w_i \\in \\mathcal{W}$ as $s_i = f_{A_i}(w_i)$.\n",
        "Then the convolution weights $w$ are modulated as follows.\n",
        "($w$ here on refers to weights not intermediate latent space,\n",
        " we are sticking to the same notation as the paper.)\n",
        "$$w'_{i, j, k} = s_i \\cdot w_{i, j, k}$$\n",
        "Then it's demodulated by normalizing,\n",
        "$$w''_{i,j,k} = \\frac{w'_{i,j,k}}{\\sqrt{\\sum_{i,k}{w'_{i, j, k}}^2 + \\epsilon}}$$\n",
        "where $i$ is the input channel, $j$ is the output channel, and $k$ is the kernel index.\n",
        "#### Path Length Regularization\n",
        "Path length regularization encourages a fixed-size step in $\\mathcal{W}$ to result in a non-zero,\n",
        " fixed-magnitude change in the generated image.\n",
        "#### No Progressive Growing\n",
        "StyleGAN2 uses residual connections (with down-sampling) in the discriminator and skip connections\n",
        " in the generator with up-sampling\n",
        "  (the RGB outputs from each layer are added - no residual connections in feature maps).\n",
        "They show that with experiments that the contribution of low-resolution layers is higher\n",
        " at beginning of the training and then high-resolution layers take over.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from typing import Tuple, Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class MappingNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"mapping_network\"></a>\n",
        "    ## Mapping Network\n",
        "    ![Mapping Network](mapping_network.svg)\n",
        "    This is an MLP with 8 linear layers.\n",
        "    The mapping network maps the latent vector $z \\in \\mathcal{W}$\n",
        "    to an intermediate latent space $w \\in \\mathcal{W}$.\n",
        "    $\\mathcal{W}$ space will be disentangled from the image space\n",
        "    where the factors of variation become more linear.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features: int, n_layers: int):\n",
        "        \"\"\"\n",
        "        * `features` is the number of features in $z$ and $w$\n",
        "        * `n_layers` is the number of layers in the mapping network.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create the MLP\n",
        "        layers = []\n",
        "        for i in range(n_layers):\n",
        "            # [Equalized learning-rate linear layers](#equalized_linear)\n",
        "            layers.append(EqualizedLinear(features, features))\n",
        "            # Leaky Relu\n",
        "            layers.append(nn.LeakyReLU(negative_slope=0.2, inplace=True))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, z: torch.Tensor):\n",
        "        # Normalize $z$\n",
        "        z = F.normalize(z, dim=1)\n",
        "        # Map $z$ to $w$\n",
        "        return self.net(z)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"generator\"></a>\n",
        "    ## StyleGAN2 Generator\n",
        "    ![Generator](style_gan2.svg)\n",
        "    ---*$A$ denotes a linear layer.\n",
        "    $B$ denotes a broadcast and scaling operation (noise is a single channel).\n",
        "    [`toRGB`](#to_rgb) also has a style modulation which is not shown in the diagram to keep it simple.*---\n",
        "    The generator starts with a learned constant.\n",
        "    Then it has a series of blocks. The feature map resolution is doubled at each block\n",
        "    Each block outputs an RGB image and they are scaled up and summed to get the final RGB image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_resolution: int, d_latent: int, n_features: int = 32, max_features: int = 512):\n",
        "        \"\"\"\n",
        "        * `log_resolution` is the $\\log_2$ of image resolution\n",
        "        * `d_latent` is the dimensionality of $w$\n",
        "        * `n_features` number of features in the convolution layer at the highest resolution (final block)\n",
        "        * `max_features` maximum number of features in any generator block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Calculate the number of features for each block\n",
        "        #\n",
        "        # Something like `[512, 512, 256, 128, 64, 32]`\n",
        "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 2, -1, -1)]\n",
        "        # Number of generator blocks\n",
        "        self.n_blocks = len(features)\n",
        "\n",
        "        # Trainable $4 \\times 4$ constant\n",
        "        self.initial_constant = nn.Parameter(torch.randn((1, features[0], 4, 4)))\n",
        "\n",
        "        # First style block for $4 \\times 4$ resolution and layer to get RGB\n",
        "        self.style_block = StyleBlock(d_latent, features[0], features[0])\n",
        "        self.to_rgb = ToRGB(d_latent, features[0])\n",
        "\n",
        "        # Generator blocks\n",
        "        blocks = [GeneratorBlock(d_latent, features[i - 1], features[i]) for i in range(1, self.n_blocks)]\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "        # $2 \\times$ up sampling layer. The feature space is up sampled\n",
        "        # at each block\n",
        "        self.up_sample = UpSample()\n",
        "\n",
        "    def forward(self, w: torch.Tensor, input_noise: List[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]]):\n",
        "        \"\"\"\n",
        "        * `w` is $w$. In order to mix-styles (use different $w$ for different layers), we provide a separate\n",
        "        $w$ for each [generator block](#generator_block). It has shape `[n_blocks, batch_size, d_latent]`.\n",
        "        * `input_noise` is the noise for each block.\n",
        "        It's a list of pairs of noise sensors because each block (except the initial) has two noise inputs\n",
        "        after each convolution layer (see the diagram).\n",
        "        \"\"\"\n",
        "\n",
        "        # Get batch size\n",
        "        batch_size = w.shape[1]\n",
        "\n",
        "        # Expand the learned constant to match batch size\n",
        "        x = self.initial_constant.expand(batch_size, -1, -1, -1)\n",
        "\n",
        "        # The first style block\n",
        "        x = self.style_block(x, w[0], input_noise[0][1])\n",
        "        # Get first rgb image\n",
        "        rgb = self.to_rgb(x, w[0])\n",
        "\n",
        "        # Evaluate rest of the blocks\n",
        "        for i in range(1, self.n_blocks):\n",
        "            # Up sample the feature map\n",
        "            x = self.up_sample(x)\n",
        "            # Run it through the [generator block](#generator_block)\n",
        "            x, rgb_new = self.blocks[i - 1](x, w[i], input_noise[i])\n",
        "            # Up sample the RGB image and add to the rgb from the block\n",
        "            rgb = self.up_sample(rgb) + rgb_new\n",
        "\n",
        "        # Return the final RGB image\n",
        "        return rgb\n",
        "\n",
        "\n",
        "class GeneratorBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"generator_block\"></a>\n",
        "    ### Generator Block\n",
        "    ![Generator block](generator_block.svg)\n",
        "    ---*$A$ denotes a linear layer.\n",
        "    $B$ denotes a broadcast and scaling operation (noise is a single channel).\n",
        "    [`toRGB`](#to_rgb) also has a style modulation which is not shown in the diagram to keep it simple.*---\n",
        "    The generator block consists of two [style blocks](#style_block) ($3 \\times 3$ convolutions with style modulation)\n",
        "    and an RGB output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_latent: int, in_features: int, out_features: int):\n",
        "        \"\"\"\n",
        "        * `d_latent` is the dimensionality of $w$\n",
        "        * `in_features` is the number of features in the input feature map\n",
        "        * `out_features` is the number of features in the output feature map\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # First [style block](#style_block) changes the feature map size to `out_features`\n",
        "        self.style_block1 = StyleBlock(d_latent, in_features, out_features)\n",
        "        # Second [style block](#style_block)\n",
        "        self.style_block2 = StyleBlock(d_latent, out_features, out_features)\n",
        "\n",
        "        # *toRGB* layer\n",
        "        self.to_rgb = ToRGB(d_latent, out_features)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, w: torch.Tensor, noise: Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        * `noise` is a tuple of two noise tensors of shape `[batch_size, 1, height, width]`\n",
        "        \"\"\"\n",
        "        # First style block with first noise tensor.\n",
        "        # The output is of shape `[batch_size, out_features, height, width]`\n",
        "        x = self.style_block1(x, w, noise[0])\n",
        "        # Second style block with second noise tensor.\n",
        "        # The output is of shape `[batch_size, out_features, height, width]`\n",
        "        x = self.style_block2(x, w, noise[1])\n",
        "\n",
        "        # Get RGB image\n",
        "        rgb = self.to_rgb(x, w)\n",
        "\n",
        "        # Return feature map and rgb image\n",
        "        return x, rgb\n",
        "\n",
        "\n",
        "class StyleBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"style_block\"></a>\n",
        "    ### Style Block\n",
        "    ![Style block](style_block.svg)\n",
        "    ---*$A$ denotes a linear layer.\n",
        "    $B$ denotes a broadcast and scaling operation (noise is single channel).*---\n",
        "    Style block has a weight modulation convolution layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_latent: int, in_features: int, out_features: int):\n",
        "        \"\"\"\n",
        "        * `d_latent` is the dimensionality of $w$\n",
        "        * `in_features` is the number of features in the input feature map\n",
        "        * `out_features` is the number of features in the output feature map\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Get style vector from $w$ (denoted by $A$ in the diagram) with\n",
        "        # an [equalized learning-rate linear layer](#equalized_linear)\n",
        "        self.to_style = EqualizedLinear(d_latent, in_features, bias=1.0)\n",
        "        # Weight modulated convolution layer\n",
        "        self.conv = Conv2dWeightModulate(in_features, out_features, kernel_size=3)\n",
        "        # Noise scale\n",
        "        self.scale_noise = nn.Parameter(torch.zeros(1))\n",
        "        # Bias\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "        # Activation function\n",
        "        self.activation = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, w: torch.Tensor, noise: Optional[torch.Tensor]):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        * `noise` is a tensor of shape `[batch_size, 1, height, width]`\n",
        "        \"\"\"\n",
        "        # Get style vector $s$\n",
        "        s = self.to_style(w)\n",
        "        # Weight modulated convolution\n",
        "        x = self.conv(x, s)\n",
        "        # Scale and add noise\n",
        "        if noise is not None:\n",
        "            x = x + self.scale_noise[None, :, None, None] * noise\n",
        "        # Add bias and evaluate activation function\n",
        "        return self.activation(x + self.bias[None, :, None, None])\n",
        "\n",
        "\n",
        "class ToRGB(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"to_rgb\"></a>\n",
        "    ### To RGB\n",
        "    ![To RGB](to_rgb.svg)\n",
        "    ---*$A$ denotes a linear layer.*---\n",
        "    Generates an RGB image from a feature map using $1 \\times 1$ convolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_latent: int, features: int):\n",
        "        \"\"\"\n",
        "        * `d_latent` is the dimensionality of $w$\n",
        "        * `features` is the number of features in the feature map\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Get style vector from $w$ (denoted by $A$ in the diagram) with\n",
        "        # an [equalized learning-rate linear layer](#equalized_linear)\n",
        "        self.to_style = EqualizedLinear(d_latent, features, bias=1.0)\n",
        "\n",
        "        # Weight modulated convolution layer without demodulation\n",
        "        self.conv = Conv2dWeightModulate(features, 3, kernel_size=1, demodulate=False)\n",
        "        # Bias\n",
        "        self.bias = nn.Parameter(torch.zeros(3))\n",
        "        # Activation function\n",
        "        self.activation = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, w: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `w` is $w$ with shape `[batch_size, d_latent]`\n",
        "        \"\"\"\n",
        "        # Get style vector $s$\n",
        "        style = self.to_style(w)\n",
        "        # Weight modulated convolution\n",
        "        x = self.conv(x, style)\n",
        "        # Add bias and evaluate activation function\n",
        "        return self.activation(x + self.bias[None, :, None, None])\n",
        "\n",
        "\n",
        "class Conv2dWeightModulate(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Convolution with Weight Modulation and Demodulation\n",
        "    This layer scales the convolution weights by the style vector and demodulates by normalizing it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, kernel_size: int,\n",
        "                 demodulate: float = True, eps: float = 1e-8):\n",
        "        \"\"\"\n",
        "        * `in_features` is the number of features in the input feature map\n",
        "        * `out_features` is the number of features in the output feature map\n",
        "        * `kernel_size` is the size of the convolution kernel\n",
        "        * `demodulate` is flag whether to normalize weights by its standard deviation\n",
        "        * `eps` is the $\\epsilon$ for normalizing\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Number of output features\n",
        "        self.out_features = out_features\n",
        "        # Whether to normalize weights\n",
        "        self.demodulate = demodulate\n",
        "        # Padding size\n",
        "        self.padding = (kernel_size - 1) // 2\n",
        "\n",
        "        # [Weights parameter with equalized learning rate](#equalized_weight)\n",
        "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
        "        # $\\epsilon$\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x: torch.Tensor, s: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` is the input feature map of shape `[batch_size, in_features, height, width]`\n",
        "        * `s` is style based scaling tensor of shape `[batch_size, in_features]`\n",
        "        \"\"\"\n",
        "\n",
        "        # Get batch size, height and width\n",
        "        b, _, h, w = x.shape\n",
        "\n",
        "        # Reshape the scales\n",
        "        s = s[:, None, :, None, None]\n",
        "        # Get [learning rate equalized weights](#equalized_weight)\n",
        "        weights = self.weight()[None, :, :, :, :]\n",
        "        # $$w`_{i,j,k} = s_i * w_{i,j,k}$$\n",
        "        # where $i$ is the input channel, $j$ is the output channel, and $k$ is the kernel index.\n",
        "        #\n",
        "        # The result has shape `[batch_size, out_features, in_features, kernel_size, kernel_size]`\n",
        "        weights = weights * s\n",
        "\n",
        "        # Demodulate\n",
        "        if self.demodulate:\n",
        "            # $$\\sigma_j = \\sqrt{\\sum_{i,k} (w'_{i, j, k})^2 + \\epsilon}$$\n",
        "            sigma_inv = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\n",
        "            # $$w''_{i,j,k} = \\frac{w'_{i,j,k}}{\\sqrt{\\sum_{i,k} (w'_{i, j, k})^2 + \\epsilon}}$$\n",
        "            weights = weights * sigma_inv\n",
        "\n",
        "        # Reshape `x`\n",
        "        x = x.reshape(1, -1, h, w)\n",
        "\n",
        "        # Reshape weights\n",
        "        _, _, *ws = weights.shape\n",
        "        weights = weights.reshape(b * self.out_features, *ws)\n",
        "\n",
        "        # Use grouped convolution to efficiently calculate the convolution with sample wise kernel.\n",
        "        # i.e. we have a different kernel (weights) for each sample in the batch\n",
        "        x = F.conv2d(x, weights, padding=self.padding, groups=b)\n",
        "\n",
        "        # Reshape `x` to `[batch_size, out_features, height, width]` and return\n",
        "        return x.reshape(-1, self.out_features, h, w)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"discriminator\"></a>\n",
        "    ## StyleGAN 2 Discriminator\n",
        "    ![Discriminator](style_gan2_disc.svg)\n",
        "    Discriminator first transforms the image to a feature map of the same resolution and then\n",
        "    runs it through a series of blocks with residual connections.\n",
        "    The resolution is down-sampled by $2 \\times$ at each block while doubling the\n",
        "    number of features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_resolution: int, n_features: int = 64, max_features: int = 512):\n",
        "        \"\"\"\n",
        "        * `log_resolution` is the $\\log_2$ of image resolution\n",
        "        * `n_features` number of features in the convolution layer at the highest resolution (first block)\n",
        "        * `max_features` maximum number of features in any generator block\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer to convert RGB image to a feature map with `n_features` number of features.\n",
        "        self.from_rgb = nn.Sequential(\n",
        "            EqualizedConv2d(3, n_features, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "\n",
        "        # Calculate the number of features for each block.\n",
        "        #\n",
        "        # Something like `[64, 128, 256, 512, 512, 512]`.\n",
        "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 1)]\n",
        "        # Number of [discirminator blocks](#discriminator_block)\n",
        "        n_blocks = len(features) - 1\n",
        "        # Discriminator blocks\n",
        "        blocks = [DiscriminatorBlock(features[i], features[i + 1]) for i in range(n_blocks)]\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        # [Mini-batch Standard Deviation](#mini_batch_std_dev)\n",
        "        self.std_dev = MiniBatchStdDev()\n",
        "        # Number of features after adding the standard deviations map\n",
        "        final_features = features[-1] + 1\n",
        "        # Final $3 \\times 3$ convolution layer\n",
        "        self.conv = EqualizedConv2d(final_features, final_features, 3)\n",
        "        # Final linear layer to get the classification\n",
        "        self.final = EqualizedLinear(2 * 2 * final_features, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` is the input image of shape `[batch_size, 3, height, width]`\n",
        "        \"\"\"\n",
        "\n",
        "        # Try to normalize the image (this is totally optional, but sped up the early training a little)\n",
        "        x = x - 0.5\n",
        "        # Convert from RGB\n",
        "        x = self.from_rgb(x)\n",
        "        # Run through the [discriminator blocks](#discriminator_block)\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Calculate and append [mini-batch standard deviation](#mini_batch_std_dev)\n",
        "        x = self.std_dev(x)\n",
        "        # $3 \\times 3$ convolution\n",
        "        x = self.conv(x)\n",
        "        # Flatten\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        # Return the classification score\n",
        "        return self.final(x)\n",
        "\n",
        "\n",
        "class DiscriminatorBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"discriminator_black\"></a>\n",
        "    ### Discriminator Block\n",
        "    ![Discriminator block](discriminator_block.svg)\n",
        "    Discriminator block consists of two $3 \\times 3$ convolutions with a residual connection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        \"\"\"\n",
        "        * `in_features` is the number of features in the input feature map\n",
        "        * `out_features` is the number of features in the output feature map\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Down-sampling and $1 \\times 1$ convolution layer for the residual connection\n",
        "        self.residual = nn.Sequential(DownSample(),\n",
        "                                      EqualizedConv2d(in_features, out_features, kernel_size=1))\n",
        "\n",
        "        # Two $3 \\times 3$ convolutions\n",
        "        self.block = nn.Sequential(\n",
        "            EqualizedConv2d(in_features, in_features, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            EqualizedConv2d(in_features, out_features, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "\n",
        "        # Down-sampling layer\n",
        "        self.down_sample = DownSample()\n",
        "\n",
        "        # Scaling factor $\\frac{1}{\\sqrt 2}$ after adding the residual\n",
        "        self.scale = 1 / math.sqrt(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get the residual connection\n",
        "        residual = self.residual(x)\n",
        "\n",
        "        # Convolutions\n",
        "        x = self.block(x)\n",
        "        # Down-sample\n",
        "        x = self.down_sample(x)\n",
        "\n",
        "        # Add the residual and scale\n",
        "        return (x + residual) * self.scale\n",
        "\n",
        "\n",
        "class MiniBatchStdDev(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"mini_batch_std_dev\"></a>\n",
        "    ### Mini-batch Standard Deviation\n",
        "    Mini-batch standard deviation calculates the standard deviation\n",
        "    across a mini-batch (or a subgroups within the mini-batch)\n",
        "    for each feature in the feature map. Then it takes the mean of all\n",
        "    the standard deviations and appends it to the feature map as one extra feature.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, group_size: int = 4):\n",
        "        \"\"\"\n",
        "        * `group_size` is the number of samples to calculate standard deviation across.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.group_size = group_size\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` is the feature map\n",
        "        \"\"\"\n",
        "        # Check if the batch size is divisible by the group size\n",
        "        assert x.shape[0] % self.group_size == 0\n",
        "        # Split the samples into groups of `group_size`, we flatten the feature map to a single dimension\n",
        "        # since we want to calculate the standard deviation for each feature.\n",
        "        grouped = x.view(self.group_size, -1)\n",
        "        # Calculate the standard deviation for each feature among `group_size` samples\n",
        "        #\n",
        "        # \\begin{align}\n",
        "        # \\mu_{i} &= \\frac{1}{N} \\sum_g x_{g,i} \\\\\n",
        "        # \\sigma_{i} &= \\sqrt{\\frac{1}{N} \\sum_g (x_{g,i} - \\mu_i)^2  + \\epsilon}\n",
        "        # \\end{align}\n",
        "        std = torch.sqrt(grouped.var(dim=0) + 1e-8)\n",
        "        # Get the mean standard deviation\n",
        "        std = std.mean().view(1, 1, 1, 1)\n",
        "        # Expand the standard deviation to append to the feature map\n",
        "        b, _, h, w = x.shape\n",
        "        std = std.expand(b, -1, h, w)\n",
        "        # Append (concatenate) the standard deviations to the feature map\n",
        "        return torch.cat([x, std], dim=1)\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"down_sample\"></a>\n",
        "    ### Down-sample\n",
        "    The down-sample operation [smoothens](#smooth) each feature channel and\n",
        "     scale $2 \\times$ using bilinear interpolation.\n",
        "    This is based on the paper\n",
        "     [Making Convolutional Networks Shift-Invariant Again](https://papers.labml.ai/paper/1904.11486).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Smoothing layer\n",
        "        self.smooth = Smooth()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Smoothing or blurring\n",
        "        x = self.smooth(x)\n",
        "        # Scaled down\n",
        "        return F.interpolate(x, (x.shape[2] // 2, x.shape[3] // 2), mode='bilinear', align_corners=False)\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"up_sample\"></a>\n",
        "    ### Up-sample\n",
        "    The up-sample operation scales the image up by $2 \\times$ and [smoothens](#smooth) each feature channel.\n",
        "    This is based on the paper\n",
        "     [Making Convolutional Networks Shift-Invariant Again](https://papers.labml.ai/paper/1904.11486).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Up-sampling layer\n",
        "        self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        # Smoothing layer\n",
        "        self.smooth = Smooth()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Up-sample and smoothen\n",
        "        return self.smooth(self.up_sample(x))\n",
        "\n",
        "\n",
        "class Smooth(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"smooth\"></a>\n",
        "    ### Smoothing Layer\n",
        "    This layer blurs each channel\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Blurring kernel\n",
        "        kernel = [[1, 2, 1],\n",
        "                  [2, 4, 2],\n",
        "                  [1, 2, 1]]\n",
        "        # Convert the kernel to a PyTorch tensor\n",
        "        kernel = torch.tensor([[kernel]], dtype=torch.float)\n",
        "        # Normalize the kernel\n",
        "        kernel /= kernel.sum()\n",
        "        # Save kernel as a fixed parameter (no gradient updates)\n",
        "        self.kernel = nn.Parameter(kernel, requires_grad=False)\n",
        "        # Padding layer\n",
        "        self.pad = nn.ReplicationPad2d(1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Get shape of the input feature map\n",
        "        b, c, h, w = x.shape\n",
        "        # Reshape for smoothening\n",
        "        x = x.view(-1, 1, h, w)\n",
        "\n",
        "        # Add padding\n",
        "        x = self.pad(x)\n",
        "\n",
        "        # Smoothen (blur) with the kernel\n",
        "        x = F.conv2d(x, self.kernel)\n",
        "\n",
        "        # Reshape and return\n",
        "        return x.view(b, c, h, w)\n",
        "\n",
        "\n",
        "class EqualizedLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"equalized_linear\"></a>\n",
        "    ## Learning-rate Equalized Linear Layer\n",
        "    This uses [learning-rate equalized weights](#equalized_weights) for a linear layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, bias: float = 0.):\n",
        "        \"\"\"\n",
        "        * `in_features` is the number of features in the input feature map\n",
        "        * `out_features` is the number of features in the output feature map\n",
        "        * `bias` is the bias initialization constant\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        # [Learning-rate equalized weights](#equalized_weights)\n",
        "        self.weight = EqualizedWeight([out_features, in_features])\n",
        "        # Bias\n",
        "        self.bias = nn.Parameter(torch.ones(out_features) * bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Linear transformation\n",
        "        return F.linear(x, self.weight(), bias=self.bias)\n",
        "\n",
        "\n",
        "class EqualizedConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"equalized_conv2d\"></a>\n",
        "    ## Learning-rate Equalized 2D Convolution Layer\n",
        "    This uses [learning-rate equalized weights](#equalized_weights) for a convolution layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int,\n",
        "                 kernel_size: int, padding: int = 0):\n",
        "        \"\"\"\n",
        "        * `in_features` is the number of features in the input feature map\n",
        "        * `out_features` is the number of features in the output feature map\n",
        "        * `kernel_size` is the size of the convolution kernel\n",
        "        * `padding` is the padding to be added on both sides of each size dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Padding size\n",
        "        self.padding = padding\n",
        "        # [Learning-rate equalized weights](#equalized_weights)\n",
        "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
        "        # Bias\n",
        "        self.bias = nn.Parameter(torch.ones(out_features))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Convolution\n",
        "        return F.conv2d(x, self.weight(), bias=self.bias, padding=self.padding)\n",
        "\n",
        "\n",
        "class EqualizedWeight(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"equalized_weight\"></a>\n",
        "    ## Learning-rate Equalized Weights Parameter\n",
        "    This is based on equalized learning rate introduced in the Progressive GAN paper.\n",
        "    Instead of initializing weights at $\\mathcal{N}(0,c)$ they initialize weights\n",
        "    to $\\mathcal{N}(0, 1)$ and then multiply them by $c$ when using it.\n",
        "    $$w_i = c \\hat{w}_i$$\n",
        "    The gradients on stored parameters $\\hat{w}$ get multiplied by $c$ but this doesn't have\n",
        "    an affect since optimizers such as Adam normalize them by a running mean of the squared gradients.\n",
        "    The optimizer updates on $\\hat{w}$ are proportionate to the learning rate $\\lambda$.\n",
        "    But the effective weights $w$ get updated proportionately to $c \\lambda$.\n",
        "    Without equalized learning rate, the effective weights will get updated proportionately to just $\\lambda$.\n",
        "    So we are effectively scaling the learning rate by $c$ for these weight parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, shape: List[int]):\n",
        "        \"\"\"\n",
        "        * `shape` is the shape of the weight parameter\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # He initialization constant\n",
        "        self.c = 1 / math.sqrt(np.prod(shape[1:]))\n",
        "        # Initialize the weights with $\\mathcal{N}(0, 1)$\n",
        "        self.weight = nn.Parameter(torch.randn(shape))\n",
        "        # Weight multiplication coefficient\n",
        "\n",
        "    def forward(self):\n",
        "        # Multiply the weights by $c$ and return\n",
        "        return self.weight * self.c\n",
        "\n",
        "\n",
        "class GradientPenalty(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"gradient_penalty\"></a>\n",
        "    ## Gradient Penalty\n",
        "    This is the $R_1$ regularization penality from the paper\n",
        "    [Which Training Methods for GANs do actually Converge?](https://papers.labml.ai/paper/1801.04406).\n",
        "    $$R_1(\\psi) = \\frac{\\gamma}{2} \\mathbb{E}_{p_\\mathcal{D}(x)}\n",
        "    \\Big[\\Vert \\nabla_x D_\\psi(x)^2 \\Vert\\Big]$$\n",
        "    That is we try to reduce the L2 norm of gradients of the discriminator with\n",
        "    respect to images, for real images ($P_\\mathcal{D}$).\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x: torch.Tensor, d: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` is $x \\sim \\mathcal{D}$\n",
        "        * `d` is $D(x)$\n",
        "        \"\"\"\n",
        "\n",
        "        # Get batch size\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Calculate gradients of $D(x)$ with respect to $x$.\n",
        "        # `grad_outputs` is set to $1$ since we want the gradients of $D(x)$,\n",
        "        # and we need to create and retain graph since we have to compute gradients\n",
        "        # with respect to weight on this loss.\n",
        "        gradients, *_ = torch.autograd.grad(outputs=d,\n",
        "                                            inputs=x,\n",
        "                                            grad_outputs=d.new_ones(d.shape),\n",
        "                                            create_graph=True)\n",
        "\n",
        "        # Reshape gradients to calculate the norm\n",
        "        gradients = gradients.reshape(batch_size, -1)\n",
        "        # Calculate the norm $\\Vert \\nabla_{x} D(x)^2 \\Vert$\n",
        "        norm = gradients.norm(2, dim=-1)\n",
        "        # Return the loss $\\Vert \\nabla_x D_\\psi(x)^2 \\Vert$\n",
        "        return torch.mean(norm ** 2)\n",
        "\n",
        "\n",
        "class PathLengthPenalty(nn.Module):\n",
        "    \"\"\"\n",
        "    <a id=\"path_length_penalty\"></a>\n",
        "    ## Path Length Penalty\n",
        "    This regularization encourages a fixed-size step in $w$ to result in a fixed-magnitude\n",
        "    change in the image.\n",
        "    $$\\mathbb{E}_{w \\sim f(z), y \\sim \\mathcal{N}(0, \\mathbf{I})}\n",
        "      \\Big(\\Vert \\mathbf{J}^\\top_{w} y \\Vert_2 - a \\Big)^2$$\n",
        "    where $\\mathbf{J}_w$ is the Jacobian\n",
        "    $\\mathbf{J}_w = \\frac{\\partial g}{\\partial w}$,\n",
        "    $w$ are sampled from $w \\in \\mathcal{W}$ from the mapping network, and\n",
        "    $y$ are images with noise $\\mathcal{N}(0, \\mathbf{I})$.\n",
        "    $a$ is the exponential moving average of $\\Vert \\mathbf{J}^\\top_{w} y \\Vert_2$\n",
        "    as the training progresses.\n",
        "    $\\mathbf{J}^\\top_{w} y$ is calculated without explicitly calculating the Jacobian using\n",
        "    $$\\mathbf{J}^\\top_{w} y = \\nabla_w \\big(g(w) \\cdot y \\big)$$\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, beta: float):\n",
        "        \"\"\"\n",
        "        * `beta` is the constant $\\beta$ used to calculate the exponential moving average $a$\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # $\\beta$\n",
        "        self.beta = beta\n",
        "        # Number of steps calculated $N$\n",
        "        self.steps = nn.Parameter(torch.tensor(0.), requires_grad=False)\n",
        "        # Exponential sum of $\\mathbf{J}^\\top_{w} y$\n",
        "        # $$\\sum^N_{i=1} \\beta^{(N - i)}[\\mathbf{J}^\\top_{w} y]_i$$\n",
        "        # where $[\\mathbf{J}^\\top_{w} y]_i$ is the value of it at $i$-th step of training\n",
        "        self.exp_sum_a = nn.Parameter(torch.tensor(0.), requires_grad=False)\n",
        "\n",
        "    def forward(self, w: torch.Tensor, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `w` is the batch of $w$ of shape `[batch_size, d_latent]`\n",
        "        * `x` are the generated images of shape `[batch_size, 3, height, width]`\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the device\n",
        "        device = x.device\n",
        "        # Get number of pixels\n",
        "        image_size = x.shape[2] * x.shape[3]\n",
        "        # Calculate $y \\in \\mathcal{N}(0, \\mathbf{I})$\n",
        "        y = torch.randn(x.shape, device=device)\n",
        "        # Calculate $\\big(g(w) \\cdot y \\big)$ and normalize by the square root of image size.\n",
        "        # This is scaling is not mentioned in the paper but was present in\n",
        "        # [their implementation](https://github.com/NVlabs/stylegan2/blob/master/training/loss.py#L167).\n",
        "        output = (x * y).sum() / math.sqrt(image_size)\n",
        "\n",
        "        # Calculate gradients to get $\\mathbf{J}^\\top_{w} y$\n",
        "        gradients, *_ = torch.autograd.grad(outputs=output,\n",
        "                                            inputs=w,\n",
        "                                            grad_outputs=torch.ones(output.shape, device=device),\n",
        "                                            create_graph=True)\n",
        "\n",
        "        # Calculate L2-norm of $\\mathbf{J}^\\top_{w} y$\n",
        "        norm = (gradients ** 2).sum(dim=2).mean(dim=1).sqrt()\n",
        "\n",
        "        # Regularize after first step\n",
        "        if self.steps > 0:\n",
        "            # Calculate $a$\n",
        "            # $$\\frac{1}{1 - \\beta^N} \\sum^N_{i=1} \\beta^{(N - i)}[\\mathbf{J}^\\top_{w} y]_i$$\n",
        "            a = self.exp_sum_a / (1 - self.beta ** self.steps)\n",
        "            # Calculate the penalty\n",
        "            # $$\\mathbb{E}_{w \\sim f(z), y \\sim \\mathcal{N}(0, \\mathbf{I})}\n",
        "            # \\Big(\\Vert \\mathbf{J}^\\top_{w} y \\Vert_2 - a \\Big)^2$$\n",
        "            loss = torch.mean((norm - a) ** 2)\n",
        "        else:\n",
        "            # Return a dummy loss if we can't calculate $a$\n",
        "            loss = norm.new_tensor(0)\n",
        "\n",
        "        # Calculate the mean of $\\Vert \\mathbf{J}^\\top_{w} y \\Vert_2$\n",
        "        mean = norm.mean().detach()\n",
        "        # Update exponential sum\n",
        "        self.exp_sum_a.mul_(self.beta).add_(mean, alpha=1 - self.beta)\n",
        "        # Increment $N$\n",
        "        self.steps.add_(1.)\n",
        "\n",
        "        # Return the penalty\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "eOjgnSS5rTm-",
        "outputId": "5ecaa3bf-06bf-4260-c3ba-79f3def471ab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\"></pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info...</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t66.26ms</span>\n",
              "</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t66.26ms</span>\n",
              "Prepare device.device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t73.41ms</span>\n",
              "</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t66.26ms</span>\n",
              "Prepare device.device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t73.41ms</span>\n",
              "</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t66.26ms</span>\n",
              "Prepare device.device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t73.41ms</span>\n",
              "\n",
              "<strong><span style=\"text-decoration: underline\">stylegan2</span></strong>: <span style=\"color: #208FFB\">c7ec11aa765711ed8d060242ac1c000c</span></pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t66.26ms</span>\n",
              "Prepare device.device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t73.41ms</span>\n",
              "\n",
              "<strong><span style=\"text-decoration: underline\">stylegan2</span></strong>: <span style=\"color: #208FFB\">c7ec11aa765711ed8d060242ac1c000c</span>\n",
              "[clean]: <strong><span style=\"color: #DDB62B\">\"\"</span></strong></pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t66.26ms</span>\n",
              "Prepare device.device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t73.41ms</span>\n",
              "\n",
              "<strong><span style=\"text-decoration: underline\">stylegan2</span></strong>: <span style=\"color: #208FFB\">c7ec11aa765711ed8d060242ac1c000c</span>\n",
              "[clean]: <strong><span style=\"color: #DDB62B\">\"\"</span></strong>\n",
              "<strong>~/labml/configs.yaml</strong> does not exist. Creating <span style=\"color: #208FFB\">/root/.labml/configs.yaml</span></pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t66.26ms</span>\n",
              "Prepare device.device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t73.41ms</span>\n",
              "\n",
              "<strong><span style=\"text-decoration: underline\">stylegan2</span></strong>: <span style=\"color: #208FFB\">c7ec11aa765711ed8d060242ac1c000c</span>\n",
              "[clean]: <strong><span style=\"color: #DDB62B\">\"\"</span></strong>\n",
              "<strong>~/labml/configs.yaml</strong> does not exist. Creating <span style=\"color: #208FFB\">/root/.labml/configs.yaml</span>\n",
              "<strong><span style=\"color: #DDB62B\">       0:  </span></strong>Discriminator:<span style=\"color: #C5C1B4\">  ...</span><span style=\"color: #208FFB\">  0ms  </span>  <span style=\"color: #208FFB\">0ms</span><span style=\"color: #D160C4\">  0:00m/  0:00m  </span></pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t66.26ms</span>\n",
              "Prepare device.device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t73.41ms</span>\n",
              "\n",
              "<strong><span style=\"text-decoration: underline\">stylegan2</span></strong>: <span style=\"color: #208FFB\">c7ec11aa765711ed8d060242ac1c000c</span>\n",
              "[clean]: <strong><span style=\"color: #DDB62B\">\"\"</span></strong>\n",
              "<strong>~/labml/configs.yaml</strong> does not exist. Creating <span style=\"color: #208FFB\">/root/.labml/configs.yaml</span>\n",
              "<strong><span style=\"color: #DDB62B\">       0:  </span></strong>Discriminator:<span style=\"color: #C5C1B4\">  ...</span><span style=\"color: #208FFB\">  0ms  </span>  <span style=\"color: #208FFB\">0ms</span><span style=\"color: #D160C4\">  0:00m/  0:00m  </span></pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t66.26ms</span>\n",
              "Prepare device.device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t73.41ms</span>\n",
              "\n",
              "<strong><span style=\"text-decoration: underline\">stylegan2</span></strong>: <span style=\"color: #208FFB\">c7ec11aa765711ed8d060242ac1c000c</span>\n",
              "[clean]: <strong><span style=\"color: #DDB62B\">\"\"</span></strong>\n",
              "<strong>~/labml/configs.yaml</strong> does not exist. Creating <span style=\"color: #208FFB\">/root/.labml/configs.yaml</span>\n",
              "<strong><span style=\"color: #DDB62B\">       0:  </span></strong>Discriminator:<span style=\"color: #C5C1B4\">  ...</span><span style=\"color: #208FFB\">  0ms  </span>  <span style=\"color: #208FFB\">0ms</span><span style=\"color: #D160C4\">  0:00m/  0:00m  </span>\n",
              "Prepare mode...</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t66.26ms</span>\n",
              "Prepare device.device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t73.41ms</span>\n",
              "\n",
              "<strong><span style=\"text-decoration: underline\">stylegan2</span></strong>: <span style=\"color: #208FFB\">c7ec11aa765711ed8d060242ac1c000c</span>\n",
              "[clean]: <strong><span style=\"color: #DDB62B\">\"\"</span></strong>\n",
              "<strong>~/labml/configs.yaml</strong> does not exist. Creating <span style=\"color: #208FFB\">/root/.labml/configs.yaml</span>\n",
              "<strong><span style=\"color: #DDB62B\">       0:  </span></strong>Discriminator:<span style=\"color: #C5C1B4\">  ...</span><span style=\"color: #208FFB\">  0ms  </span>  <span style=\"color: #208FFB\">0ms</span><span style=\"color: #D160C4\">  0:00m/  0:00m  </span>\n",
              "Prepare mode<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t9.17ms</span>\n",
              "</pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t66.26ms</span>\n",
              "Prepare device.device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t73.41ms</span>\n",
              "\n",
              "<strong><span style=\"text-decoration: underline\">stylegan2</span></strong>: <span style=\"color: #208FFB\">c7ec11aa765711ed8d060242ac1c000c</span>\n",
              "[clean]: <strong><span style=\"color: #DDB62B\">\"\"</span></strong>\n",
              "<strong>~/labml/configs.yaml</strong> does not exist. Creating <span style=\"color: #208FFB\">/root/.labml/configs.yaml</span>\n",
              "<strong><span style=\"color: #DDB62B\">       0:  </span></strong>Discriminator:<span style=\"color: #C5C1B4\">  ...</span><span style=\"color: #208FFB\">  0ms  </span>  <span style=\"color: #208FFB\">0ms</span><span style=\"color: #D160C4\">  0:00m/  0:00m  </span>\n",
              "Prepare mode<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t9.17ms</span>\n",
              "<span style=\"color: #C5C1B4\"></span>\n",
              "<span style=\"color: #C5C1B4\">--------------------------------------------------</span><span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>\n",
              "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\">LABML WARNING</span></strong></span>\n",
              "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>LabML App Warning: <span style=\"color: #60C6C8\">empty_token: </span><strong>Please create a valid token at https://app.labml.ai.</strong>\n",
              "<strong>Click on the experiment link to monitor the experiment and add it to your experiments list.</strong><span style=\"color: #C5C1B4\"></span>\n",
              "<span style=\"color: #C5C1B4\">--------------------------------------------------</span></pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"overflow-x: scroll;\">\n",
              "Prepare device.device...\n",
              "  Prepare device.device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t66.26ms</span>\n",
              "Prepare device.device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t73.41ms</span>\n",
              "\n",
              "<strong><span style=\"text-decoration: underline\">stylegan2</span></strong>: <span style=\"color: #208FFB\">c7ec11aa765711ed8d060242ac1c000c</span>\n",
              "[clean]: <strong><span style=\"color: #DDB62B\">\"\"</span></strong>\n",
              "<strong>~/labml/configs.yaml</strong> does not exist. Creating <span style=\"color: #208FFB\">/root/.labml/configs.yaml</span>\n",
              "<strong><span style=\"color: #DDB62B\">       0:  </span></strong>Discriminator:<span style=\"color: #C5C1B4\">  ...</span><span style=\"color: #208FFB\">  0ms  </span>  <span style=\"color: #208FFB\">0ms</span><span style=\"color: #D160C4\">  0:00m/  0:00m  </span>\n",
              "Prepare mode<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t9.17ms</span>\n",
              "<span style=\"color: #C5C1B4\"></span>\n",
              "<span style=\"color: #C5C1B4\">--------------------------------------------------</span><span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>\n",
              "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\">LABML WARNING</span></strong></span>\n",
              "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>LabML App Warning: <span style=\"color: #60C6C8\">empty_token: </span><strong>Please create a valid token at https://app.labml.ai.</strong>\n",
              "<strong>Click on the experiment link to monitor the experiment and add it to your experiments list.</strong><span style=\"color: #C5C1B4\"></span>\n",
              "<span style=\"color: #C5C1B4\">--------------------------------------------------</span>\n",
              "<span style=\"color: #208FFB\">Monitor experiment at </span><a href='https://app.labml.ai/run/c7ec11aa765711ed8d060242ac1c000c' target='blank'>https://app.labml.ai/run/c7ec11aa765711ed8d060242ac1c000c</a></pre>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\"\n",
        "---\n",
        "title: StyleGAN 2 Model Training\n",
        "summary: >\n",
        " An annotated PyTorch implementation of StyleGAN2 model training code.\n",
        "---\n",
        "# [StyleGAN 2](index.html) Model Training\n",
        "This is the training code for [StyleGAN 2](index.html) model.\n",
        "![Generated Images](generated_64.png)\n",
        "---*These are $64 \\times 64$ images generated after training for about 80K steps.*---\n",
        "*Our implementation is a minimalistic StyleGAN 2 model training code.\n",
        "Only single GPU training is supported to keep the implementation simple.\n",
        "We managed to shrink it to keep it at less than 500 lines of code, including the training loop.*\n",
        "*Without DDP (distributed data parallel) and multi-gpu training it will not be possible to train the model\n",
        "for large resolutions (128+).\n",
        "If you want training code with fp16 and DDP take a look at\n",
        "[lucidrains/stylegan2-pytorch](https://github.com/lucidrains/stylegan2-pytorch).*\n",
        "We trained this on [CelebA-HQ dataset](https://github.com/tkarras/progressive_growing_of_gans).\n",
        "You can find the download instruction in this\n",
        "[discussion on fast.ai](https://forums.fast.ai/t/download-celeba-hq-dataset/45873/3).\n",
        "Save the images inside [`data/stylegan` folder](#dataset_path).\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Iterator, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "\n",
        "from labml import tracker, lab, monit, experiment\n",
        "from labml.configs import BaseConfigs\n",
        "from labml_helpers.device import DeviceConfigs\n",
        "from labml_helpers.train_valid import ModeState, hook_model_outputs\n",
        "from labml_nn.gan.stylegan import Discriminator, Generator, MappingNetwork, GradientPenalty, PathLengthPenalty\n",
        "from labml_nn.gan.wasserstein import DiscriminatorLoss, GeneratorLoss\n",
        "from labml_nn.utils import cycle_dataloader\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    ## Dataset\n",
        "    This loads the training dataset and resize it to the give image size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path: str, image_size: int):\n",
        "        \"\"\"\n",
        "        * `path` path to the folder containing the images\n",
        "        * `image_size` size of the image\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Get the paths of all `jpg` files\n",
        "        self.paths = [p for p in Path(path).glob(f'**/*.jpg')]\n",
        "\n",
        "        # Transformation\n",
        "        self.transform = torchvision.transforms.Compose([\n",
        "            # Resize the image\n",
        "            torchvision.transforms.Resize(image_size),\n",
        "            # Convert to PyTorch tensor\n",
        "            torchvision.transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of images\"\"\"\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get the the `index`-th image\"\"\"\n",
        "        path = self.paths[index]\n",
        "        img = Image.open(path)\n",
        "        return self.transform(img)\n",
        "\n",
        "\n",
        "class Configs(BaseConfigs):\n",
        "    \"\"\"\n",
        "    ## Configurations\n",
        "    \"\"\"\n",
        "\n",
        "    # Device to train the model on.\n",
        "    # [`DeviceConfigs`](https://docs.labml.ai/api/helpers.html#labml_helpers.device.DeviceConfigs)\n",
        "    #  picks up an available CUDA device or defaults to CPU.\n",
        "    device: torch.device = DeviceConfigs()\n",
        "\n",
        "    # [StyleGAN2 Discriminator](index.html#discriminator)\n",
        "    discriminator: Discriminator\n",
        "    # [StyleGAN2 Generator](index.html#generator)\n",
        "    generator: Generator\n",
        "    # [Mapping network](index.html#mapping_network)\n",
        "    mapping_network: MappingNetwork\n",
        "\n",
        "    # Discriminator and generator loss functions.\n",
        "    # We use [Wasserstein loss](../wasserstein/index.html)\n",
        "    discriminator_loss: DiscriminatorLoss\n",
        "    generator_loss: GeneratorLoss\n",
        "\n",
        "    # Optimizers\n",
        "    generator_optimizer: torch.optim.Adam\n",
        "    discriminator_optimizer: torch.optim.Adam\n",
        "    mapping_network_optimizer: torch.optim.Adam\n",
        "\n",
        "    # [Gradient Penalty Regularization Loss](index.html#gradient_penalty)\n",
        "    gradient_penalty = GradientPenalty()\n",
        "    # Gradient penalty coefficient $\\gamma$\n",
        "    gradient_penalty_coefficient: float = 10.\n",
        "\n",
        "    # [Path length penalty](index.html#path_length_penalty)\n",
        "    path_length_penalty: PathLengthPenalty\n",
        "\n",
        "    # Data loader\n",
        "    loader: Iterator\n",
        "\n",
        "    # Batch size\n",
        "    batch_size: int = 32\n",
        "    # Dimensionality of $z$ and $w$\n",
        "    d_latent: int = 512\n",
        "    # Height/width of the image\n",
        "    image_size: int = 32\n",
        "    # Number of layers in the mapping network\n",
        "    mapping_network_layers: int = 8\n",
        "    # Generator & Discriminator learning rate\n",
        "    learning_rate: float = 1e-3\n",
        "    # Mapping network learning rate ($100 \\times$ lower than the others)\n",
        "    mapping_network_learning_rate: float = 1e-5\n",
        "    # Number of steps to accumulate gradients on. Use this to increase the effective batch size.\n",
        "    gradient_accumulate_steps: int = 1\n",
        "    # $\\beta_1$ and $\\beta_2$ for Adam optimizer\n",
        "    adam_betas: Tuple[float, float] = (0.0, 0.99)\n",
        "    # Probability of mixing styles\n",
        "    style_mixing_prob: float = 0.9\n",
        "\n",
        "    # Total number of training steps\n",
        "    training_steps: int = 150_000\n",
        "\n",
        "    # Number of blocks in the generator (calculated based on image resolution)\n",
        "    n_gen_blocks: int\n",
        "\n",
        "    # ### Lazy regularization\n",
        "    # Instead of calculating the regularization losses, the paper proposes lazy regularization\n",
        "    # where the regularization terms are calculated once in a while.\n",
        "    # This improves the training efficiency a lot.\n",
        "\n",
        "    # The interval at which to compute gradient penalty\n",
        "    lazy_gradient_penalty_interval: int = 4\n",
        "    # Path length penalty calculation interval\n",
        "    lazy_path_penalty_interval: int = 32\n",
        "    # Skip calculating path length penalty during the initial phase of training\n",
        "    lazy_path_penalty_after: int = 5_000\n",
        "\n",
        "    # How often to log generated images\n",
        "    log_generated_interval: int = 500\n",
        "    # How often to save model checkpoints\n",
        "    save_checkpoint_interval: int = 2_000\n",
        "\n",
        "    # Training mode state for logging activations\n",
        "    mode: ModeState\n",
        "    # Whether to log model layer outputs\n",
        "    log_layer_outputs: bool = False\n",
        "\n",
        "    # <a id=\"dataset_path\"></a>\n",
        "    # We trained this on [CelebA-HQ dataset](https://github.com/tkarras/progressive_growing_of_gans).\n",
        "    # You can find the download instruction in this\n",
        "    # [discussion on fast.ai](https://forums.fast.ai/t/download-celeba-hq-dataset/45873/3).\n",
        "    # Save the images inside `data/stylegan` folder.\n",
        "    dataset_path: str = str(lab.get_data_path() / 'stylegan2')\n",
        "\n",
        "    def init(self):\n",
        "        \"\"\"\n",
        "        ### Initialize\n",
        "        \"\"\"\n",
        "        # Create dataset\n",
        "        dataset = Dataset(self.dataset_path, self.image_size)\n",
        "        # Create data loader\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, num_workers=8,\n",
        "                                                 shuffle=True, drop_last=True, pin_memory=True)\n",
        "        # Continuous [cyclic loader](../../utils.html#cycle_dataloader)\n",
        "        self.loader = cycle_dataloader(dataloader)\n",
        "\n",
        "        # $\\log_2$ of image resolution\n",
        "        log_resolution = int(math.log2(self.image_size))\n",
        "\n",
        "        # Create discriminator and generator\n",
        "        self.discriminator = Discriminator(log_resolution).to(self.device)\n",
        "        self.generator = Generator(log_resolution, self.d_latent).to(self.device)\n",
        "        # Get number of generator blocks for creating style and noise inputs\n",
        "        self.n_gen_blocks = self.generator.n_blocks\n",
        "        # Create mapping network\n",
        "        self.mapping_network = MappingNetwork(self.d_latent, self.mapping_network_layers).to(self.device)\n",
        "        # Create path length penalty loss\n",
        "        self.path_length_penalty = PathLengthPenalty(0.99).to(self.device)\n",
        "\n",
        "        # Add model hooks to monitor layer outputs\n",
        "        if self.log_layer_outputs:\n",
        "            hook_model_outputs(self.mode, self.discriminator, 'discriminator')\n",
        "            hook_model_outputs(self.mode, self.generator, 'generator')\n",
        "            hook_model_outputs(self.mode, self.mapping_network, 'mapping_network')\n",
        "\n",
        "        # Discriminator and generator losses\n",
        "        self.discriminator_loss = DiscriminatorLoss().to(self.device)\n",
        "        self.generator_loss = GeneratorLoss().to(self.device)\n",
        "\n",
        "        # Create optimizers\n",
        "        self.discriminator_optimizer = torch.optim.Adam(\n",
        "            self.discriminator.parameters(),\n",
        "            lr=self.learning_rate, betas=self.adam_betas\n",
        "        )\n",
        "        self.generator_optimizer = torch.optim.Adam(\n",
        "            self.generator.parameters(),\n",
        "            lr=self.learning_rate, betas=self.adam_betas\n",
        "        )\n",
        "        self.mapping_network_optimizer = torch.optim.Adam(\n",
        "            self.mapping_network.parameters(),\n",
        "            lr=self.mapping_network_learning_rate, betas=self.adam_betas\n",
        "        )\n",
        "\n",
        "        # Set tracker configurations\n",
        "        tracker.set_image(\"generated\", True)\n",
        "\n",
        "    def get_w(self, batch_size: int):\n",
        "        \"\"\"\n",
        "        ### Sample $w$\n",
        "        This samples $z$ randomly and get $w$ from the mapping network.\n",
        "        We also apply style mixing sometimes where we generate two latent variables\n",
        "        $z_1$ and $z_2$ and get corresponding $w_1$ and $w_2$.\n",
        "        Then we randomly sample a cross-over point and apply $w_1$ to\n",
        "        the generator blocks before the cross-over point and\n",
        "        $w_2$ to the blocks after.\n",
        "        \"\"\"\n",
        "\n",
        "        # Mix styles\n",
        "        if torch.rand(()).item() < self.style_mixing_prob:\n",
        "            # Random cross-over point\n",
        "            cross_over_point = int(torch.rand(()).item() * self.n_gen_blocks)\n",
        "            # Sample $z_1$ and $z_2$\n",
        "            z2 = torch.randn(batch_size, self.d_latent).to(self.device)\n",
        "            z1 = torch.randn(batch_size, self.d_latent).to(self.device)\n",
        "            # Get $w_1$ and $w_2$\n",
        "            w1 = self.mapping_network(z1)\n",
        "            w2 = self.mapping_network(z2)\n",
        "            # Expand $w_1$ and $w_2$ for the generator blocks and concatenate\n",
        "            w1 = w1[None, :, :].expand(cross_over_point, -1, -1)\n",
        "            w2 = w2[None, :, :].expand(self.n_gen_blocks - cross_over_point, -1, -1)\n",
        "            return torch.cat((w1, w2), dim=0)\n",
        "        # Without mixing\n",
        "        else:\n",
        "            # Sample $z$ and $z$\n",
        "            z = torch.randn(batch_size, self.d_latent).to(self.device)\n",
        "            # Get $w$ and $w$\n",
        "            w = self.mapping_network(z)\n",
        "            # Expand $w$ for the generator blocks\n",
        "            return w[None, :, :].expand(self.n_gen_blocks, -1, -1)\n",
        "\n",
        "    def get_noise(self, batch_size: int):\n",
        "        \"\"\"\n",
        "        ### Generate noise\n",
        "        This generates noise for each [generator block](index.html#generator_block)\n",
        "        \"\"\"\n",
        "        # List to store noise\n",
        "        noise = []\n",
        "        # Noise resolution starts from $4$\n",
        "        resolution = 4\n",
        "\n",
        "        # Generate noise for each generator block\n",
        "        for i in range(self.n_gen_blocks):\n",
        "            # The first block has only one $3 \\times 3$ convolution\n",
        "            if i == 0:\n",
        "                n1 = None\n",
        "            # Generate noise to add after the first convolution layer\n",
        "            else:\n",
        "                n1 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n",
        "            # Generate noise to add after the second convolution layer\n",
        "            n2 = torch.randn(batch_size, 1, resolution, resolution, device=self.device)\n",
        "\n",
        "            # Add noise tensors to the list\n",
        "            noise.append((n1, n2))\n",
        "\n",
        "            # Next block has $2 \\times$ resolution\n",
        "            resolution *= 2\n",
        "\n",
        "        # Return noise tensors\n",
        "        return noise\n",
        "\n",
        "    def generate_images(self, batch_size: int):\n",
        "        \"\"\"\n",
        "        ### Generate images\n",
        "        This generate images using the generator\n",
        "        \"\"\"\n",
        "\n",
        "        # Get $w$\n",
        "        w = self.get_w(batch_size)\n",
        "        # Get noise\n",
        "        noise = self.get_noise(batch_size)\n",
        "\n",
        "        # Generate images\n",
        "        images = self.generator(w, noise)\n",
        "\n",
        "        # Return images and $w$\n",
        "        return images, w\n",
        "\n",
        "    def step(self, idx: int):\n",
        "        \"\"\"\n",
        "        ### Training Step\n",
        "        \"\"\"\n",
        "\n",
        "        # Train the discriminator\n",
        "        with monit.section('Discriminator'):\n",
        "            # Reset gradients\n",
        "            self.discriminator_optimizer.zero_grad()\n",
        "\n",
        "            # Accumulate gradients for `gradient_accumulate_steps`\n",
        "            for i in range(self.gradient_accumulate_steps):\n",
        "                # Update `mode`. Set whether to log activation\n",
        "                with self.mode.update(is_log_activations=(idx + 1) % self.log_generated_interval == 0):\n",
        "                    # Sample images from generator\n",
        "                    generated_images, _ = self.generate_images(self.batch_size)\n",
        "                    # Discriminator classification for generated images\n",
        "                    fake_output = self.discriminator(generated_images.detach())\n",
        "\n",
        "                    # Get real images from the data loader\n",
        "                    real_images = next(self.loader).to(self.device)\n",
        "                    # We need to calculate gradients w.r.t. real images for gradient penalty\n",
        "                    if (idx + 1) % self.lazy_gradient_penalty_interval == 0:\n",
        "                        real_images.requires_grad_()\n",
        "                    # Discriminator classification for real images\n",
        "                    real_output = self.discriminator(real_images)\n",
        "\n",
        "                    # Get discriminator loss\n",
        "                    real_loss, fake_loss = self.discriminator_loss(real_output, fake_output)\n",
        "                    disc_loss = real_loss + fake_loss\n",
        "\n",
        "                    # Add gradient penalty\n",
        "                    if (idx + 1) % self.lazy_gradient_penalty_interval == 0:\n",
        "                        # Calculate and log gradient penalty\n",
        "                        gp = self.gradient_penalty(real_images, real_output)\n",
        "                        tracker.add('loss.gp', gp)\n",
        "                        # Multiply by coefficient and add gradient penalty\n",
        "                        disc_loss = disc_loss + 0.5 * self.gradient_penalty_coefficient * gp * self.lazy_gradient_penalty_interval\n",
        "\n",
        "                    # Compute gradients\n",
        "                    disc_loss.backward()\n",
        "\n",
        "                    # Log discriminator loss\n",
        "                    tracker.add('loss.discriminator', disc_loss)\n",
        "\n",
        "            if (idx + 1) % self.log_generated_interval == 0:\n",
        "                # Log discriminator model parameters occasionally\n",
        "                tracker.add('discriminator', self.discriminator)\n",
        "\n",
        "            # Clip gradients for stabilization\n",
        "            torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), max_norm=1.0)\n",
        "            # Take optimizer step\n",
        "            self.discriminator_optimizer.step()\n",
        "\n",
        "        # Train the generator\n",
        "        with monit.section('Generator'):\n",
        "            # Reset gradients\n",
        "            self.generator_optimizer.zero_grad()\n",
        "            self.mapping_network_optimizer.zero_grad()\n",
        "\n",
        "            # Accumulate gradients for `gradient_accumulate_steps`\n",
        "            for i in range(self.gradient_accumulate_steps):\n",
        "                # Sample images from generator\n",
        "                generated_images, w = self.generate_images(self.batch_size)\n",
        "                # Discriminator classification for generated images\n",
        "                fake_output = self.discriminator(generated_images)\n",
        "\n",
        "                # Get generator loss\n",
        "                gen_loss = self.generator_loss(fake_output)\n",
        "\n",
        "                # Add path length penalty\n",
        "                if idx > self.lazy_path_penalty_after and (idx + 1) % self.lazy_path_penalty_interval == 0:\n",
        "                    # Calculate path length penalty\n",
        "                    plp = self.path_length_penalty(w, generated_images)\n",
        "                    # Ignore if `nan`\n",
        "                    if not torch.isnan(plp):\n",
        "                        tracker.add('loss.plp', plp)\n",
        "                        gen_loss = gen_loss + plp\n",
        "\n",
        "                # Calculate gradients\n",
        "                gen_loss.backward()\n",
        "\n",
        "                # Log generator loss\n",
        "                tracker.add('loss.generator', gen_loss)\n",
        "\n",
        "            if (idx + 1) % self.log_generated_interval == 0:\n",
        "                # Log discriminator model parameters occasionally\n",
        "                tracker.add('generator', self.generator)\n",
        "                tracker.add('mapping_network', self.mapping_network)\n",
        "\n",
        "            # Clip gradients for stabilization\n",
        "            torch.nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(self.mapping_network.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Take optimizer step\n",
        "            self.generator_optimizer.step()\n",
        "            self.mapping_network_optimizer.step()\n",
        "\n",
        "        # Log generated images\n",
        "        if (idx + 1) % self.log_generated_interval == 0:\n",
        "            tracker.add('generated', torch.cat([generated_images[:6], real_images[:3]], dim=0))\n",
        "        # Save model checkpoints\n",
        "        if (idx + 1) % self.save_checkpoint_interval == 0:\n",
        "            experiment.save_checkpoint()\n",
        "\n",
        "        # Flush tracker\n",
        "        tracker.save()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        ## Train model\n",
        "        \"\"\"\n",
        "\n",
        "        # Loop for `training_steps`\n",
        "        for i in monit.loop(self.training_steps):\n",
        "            # Take a training step\n",
        "            self.step(i)\n",
        "            #\n",
        "            if (i + 1) % self.log_generated_interval == 0:\n",
        "                tracker.new_line()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    ### Train StyleGAN2\n",
        "    \"\"\"\n",
        "\n",
        "    # Create an experiment\n",
        "    experiment.create(name='stylegan2')\n",
        "    # Create configurations object\n",
        "    configs = Configs()\n",
        "\n",
        "    # Set configurations and override some\n",
        "    experiment.configs(configs, {\n",
        "        'device.cuda_device': 0,\n",
        "        'image_size': 64,\n",
        "        'log_generated_interval': 200\n",
        "    })\n",
        "\n",
        "    # Initialize\n",
        "    configs.init()\n",
        "    # Set models for saving and loading\n",
        "    experiment.add_pytorch_models(mapping_network=configs.mapping_network,\n",
        "                                  generator=configs.generator,\n",
        "                                  discriminator=configs.discriminator)\n",
        "\n",
        "    # Start the experiment\n",
        "    with experiment.start():\n",
        "        # Run the training loop\n",
        "        configs.train()\n",
        "\n",
        "\n",
        "#\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}